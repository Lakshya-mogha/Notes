This will always be taught as the first algo.

## Problem we are solving 

It is a regression problem statement. 

| Input       | output      |
| ----------- | ----------- |
| weight (kg) | height (cm) |
| 73          | 150         |
| 63          | 155         |
| 78          | 170         |
| 82          | 165         |

```chart
type: line
labels: [63,73,78,82]
series:
  - title: height
    data: [155,150,170,165]
tension: 0.2
width: 80%
labelColors: false
fill: false
beginAtZero: false
bestFit: true
bestFitTitle: best fit
bestFitNumber: 0

```

The machine learning model will create a best fit linear line around the given data points. whenever the model will be given a weight it will look through the best fit line and will git the corresponding height as the output.

![[Pasted image 20241202102747.png]]

==Aim is to find best fit line with minimum residual error or the distance between x and the best fit line==

---
## Maths

The regression line or the best fit line can be represented by $y = mx + c$

where m = slope or coefficient, c = Intercept

==Equation for hypothesis== 

$$ h_\theta(x)^{(i)} = \theta_0 + \theta_1x^{(i)} $$

This equation can be changed according to the no. of inputs features to draw the best line.

For one input feature -> $h_\theta(x) = \theta_0 + \theta_1 x_1$ 

For (n) amount of features -> $h_\theta(x) = \theta_0 + \theta_1 x_1 ........ \theta_n x_n$

As we know y is dependent feature so in this case we will say *y is a linear function of x*.

---
### Example 

![[Pasted image 20241203100949.png]]

--- 
### Cost function 

We will determine the error or the distance between the best fit line and the the x input. 

Our task is to determine the value of $\theta$ such that the value of cost function is closer to zero.

$$ \sum_{i=1}^m \frac1{2m} (h_\theta(x)^{(i)} - y^{(i)})^2 $$

Can also be written as $$ j(\theta_0,\theta_1) = \frac1{2m} \sum_{i=1}^m(h_\theta(x)^{(i)} - y^{(i)})^2 $$ also known as Squares Error function

![[Pasted image 20241203105312.png]]

We are supposed to train our model till we reach close global minima. 

So how do we reach the global minima, we have to update the value of theta such that we can come closer to the global minima. 

But how come we keep changing the value of theta?

Firstly we have to find out the slope and derivative of the gradient descent.

we use ==Repeat convergence Theorem==.$$ \theta_j := \theta_j - \alpha \frac d{d\theta_j}(j(\theta_1)) $$ where $\alpha$ is Learning rate , $j(\theta_1)$ is value of cost function.

In the equation above, $\frac d{d\theta_j}(j(\theta_1))$ is the slope of gradient descent. If the slope is $+ve$ then the value of $\theta$ will decrease and if the value is $-ve$ the value of $\theta$ will increase. 

The repeat convergence theorem will run continuously to find if the $\theta$ should be incremented or decremented to get us more closer to the global minima which will eventually decrease the error rate. 

The learning rate or $\alpha$ determine the change in value of $\theta$. which determines the no of iterations needed to get near the global minima. 

If it is too small it will take to many iteration but eventually will reach global minima. If it is too big it will start jumping from $+ve$ to $-ve$ slope and may never reach global minima.

## Outline 

1.  Start with some $\theta_0$ & $\theta_1$.
2. Keep changing $\theta_0\theta_1$ to reduce $j(\theta_0,\theta_1)$ until we reach global minima.
3. Use Convergence Theorem

