

## ğŸ”¹ Learning Objectives
- Understanding **Classification**
- Implementing **NaÃ¯ve Bayes Classifier**
- Introduction to **k-Nearest Neighbors (kNN) Classifier**

---

## ğŸ”¹ What is Classification?
### ğŸ“Œ Definition

Classification is a supervised learning task where the goal is to predict **discrete class labels** for given input features.

- If **C = 2**, it is **Binary Classification** (e.g., Spam or Not Spam).
- If **C > 2**, it is **Multi-Class Classification** (e.g., Dog, Cat, or Bird).

### ğŸ“Œ Steps in Classification

1ï¸âƒ£ **Collect labeled training data**  
2ï¸âƒ£ **Split into training and test sets**  
3ï¸âƒ£ **Choose a classification algorithm**  
4ï¸âƒ£ **Train the model on the training set**  
5ï¸âƒ£ **Test the model on unseen data**  

---

## ğŸ”¹ NaÃ¯ve Bayes Classifier
### ğŸ“Œ What is NaÃ¯ve Bayes?

A **probabilistic classifier** based on **Bayes' Theorem**, assuming **independent features**:

\[ P(Y|X) = \frac{P(X|Y) P(Y)}{P(X)} \]

Where:

- \( P(Y|X) \) â†’ Posterior Probability (probability of class Y given features X)
- \( P(X|Y) \) â†’ Likelihood (probability of X given class Y)
- \( P(Y) \) â†’ Prior Probability (probability of class Y before seeing data)
- \( P(X) \) â†’ Evidence (total probability of data X)

### ğŸ“Œ Python Example: NaÃ¯ve Bayes for Classification
```python
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np

# Sample dataset (Age, Income) â†’ (Buys Computer: Yes/No)
X = np.array([[25, 50000], [30, 60000], [35, 80000], [40, 100000], [45, 120000]])
y = np.array([0, 0, 1, 1, 1])  # 0 = No, 1 = Yes

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train NaÃ¯ve Bayes model
model = GaussianNB()
model.fit(X_train, y_train)

# Predict on test data
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
```

### ğŸ“Œ Pros & Cons of NaÃ¯ve Bayes

âœ… **Fast & Efficient** for large datasets  
âœ… **Works well for categorical data**  
âŒ **Assumes independence** between features (not always true)  
âŒ **Performs poorly with small datasets**  

---

## ğŸ”¹ k-Nearest Neighbors (kNN) Classifier
### ğŸ“Œ What is kNN?

kNN is a **distance-based classifier** that assigns a class label based on **majority voting** of the **k-nearest neighbors**.

### ğŸ“Œ Steps in kNN Classification

1ï¸âƒ£ **Choose k (number of neighbors)**  
2ï¸âƒ£ **Calculate distances** from test point to all training points  
3ï¸âƒ£ **Select k closest points**  
4ï¸âƒ£ **Assign class label based on majority vote**  

### ğŸ“Œ Python Example: kNN Classifier
```python
from sklearn.neighbors import KNeighborsClassifier

# Create kNN classifier with k=3
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# Predict on test data
y_pred_knn = knn.predict(X_test)
print("kNN Accuracy:", accuracy_score(y_test, y_pred_knn))
```

### ğŸ“Œ Distance Metrics in kNN
- **Euclidean Distance** (default): \( d = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2} \)
- **Manhattan Distance**: \( d = |x_1 - x_2| + |y_1 - y_2| \)

### ğŸ“Œ Pros & Cons of kNN

âœ… **Simple & Intuitive**  
âœ… **Works well for non-linear decision boundaries**  
âŒ **Computationally expensive for large datasets**  
âŒ **Sensitive to outliers**  

---

## ğŸ”¹ Summary

âœ… **Classification predicts discrete class labels**  
âœ… **NaÃ¯ve Bayes** is a **probabilistic classifier** using **Bayes' Theorem**  
âœ… **kNN** is a **distance-based classifier** using **majority voting**  
